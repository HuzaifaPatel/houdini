\section{Related Work}%
\label{sec:related}

% general motivation for more repeatable/comparable experiments~\cite{killourhy_should_2011}. We have a very specific/concrete use case (unlike~\cite{thorpe_verification_2022} or \cite{dashevskyi_testrex_2014}) - compare security of containers. networking is the main focus, now we're looking at hosts/containers. 

To our knowledge, \houdini{} is the first comprehensive approach for evaluating and comparing \textit{container confinement mechanisms}. Much of the container security literature to date has focused on either vulnerability scanning of container images, building offensive/attack tools for container escapes, or proposing best practices for securing container runtimes. While these papers, tools, and documents do not directly share our research objectives, they still broadly fit into the container security landscape. The remainder of this section does not aim to exhaustively enumerate all container security tools and systems, but rather to highlight the various salient approaches and contrast them to \houdini.

\noindent\textbf{Repeatable Exploit Frameworks.} 
TestREX~\cite{dashevskyi_testrex_2014} - repeatable exploits in different software versions. 

\noindent\textbf{Vulnerability Scanners and measurement studies.} Many free, open and closed source tools exist for identifying the presence of known vulnerabilities in container images. Shu \etal~\cite{shu_study_2017} developed a vulnerability scanning tool to scan Docker Hub container images at scale. They found that on average, official and community images have concerning amounts of vulnerabilities (180+), and that these vulnerabilities remain unpatched for hundreds of days. This motivates the need for container confinement. Lin \etal~\cite{lin_measurement_2018} collected a dataset of 223 container exploits to create a taxonomy of attacks and propose a lightweight mitigation strategy. 

Many commercial tools aim to automate the search for known vulnerabilities (often those reported \todo{through the CVE process}). IBM Vulnerability advisor\footnote{\url{https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=guide-vulnerability-advisor}} scans images when they are uploaded to IBM's container registry. Red Hat's Clair\footnote{\url{https://www.redhat.com/en/topics/containers/what-is-clair}} statically inspects images layer by layer. Anchore's container registry scanner\footnote{\url{https://anchore.com/container-registry-scanning/}} can monitor a range of container registries, and block deployment if the image does not meet a security policy. Javed and Toor~\cite{javed_evaluation_2021} evaluate the accuracy of several commercial scanners and find that many vulnerabilities can go undetected due to the static analysis nature of the tools.

\houdini analyzes and reports on container confinement, so application-specific vulnerabilities that do not result in a container escape are not in scope. However, we expect \houdini trick developers will largely base their contributions on the CVE framework like the tools described above.
% Undiscussed above:
% Banyan collector https://github.com/banyanops/collector - Docker only, used for static analysis of container images, behavioral properties cannot be tested directly as in Houdini.
% OpenSCAP

\noindent\textbf{Offensive Tools.} The offensive security community has developed tools to attack containers. CDK\footnote{\url{https://github.com/cdk-team/CDK}} and DEEPCE\footnote{\url{https://github.com/stealthcopter/deepce}} are two popular open source container penetration testing toolkits that attempt a collection of known exploits to gain persistence, escape the container, or gather information about the environment. \houdini takes a more systematic approach that allows direct comparison of defensive techniques by clearly defining the environment, exploit steps, and output, whereas CDK's goal is to use any possible technique to bypass the environment's defenses.

% hacking guide (offensive)
%  - https://book.hacktricks.xyz/linux-hardening/privilege-escalation/docker-breakout/docker-breakout-privilege-escalation
% - https://github.com/stealthcopter/deepce (docker specific escape scripts)

\noindent\textbf{Best Practices.} Users in search of (often high-level) advice on how to improve the security of their container deployments can consult one of many guides~\cite{us_national_security_agency_kubernetes_2022,segura_dockerfile_2021,owasp_docker_nodate} available online. In our experience, guides tend to offer generic advice (e.g., ``\textit{Only allow read access to the root filesystem}''~\cite{segura_dockerfile_2021} or ``\textit{Use Linux Security Module (seccomp, AppArmor, or SELinux)}''~\cite{owasp_docker_nodate}), making the advice difficult to follow. Guides, by their nature also tend to lack sufficient context to assist administrators in deploying the advice within their specific environments.

\todo{\houdini could be used to systematically compare best practices from several sources.... We leave this for future work.}


%gap we're filling: rather than checking if existing software frameworks are configured properly (docker/apparmor/known unknowns) - we make no assumptions about how the protection works in Houdini. We need this because new frameworks aren't supported by runtime (or enforcement)-specific best practices. Houdini is a defensive evaluation (methods of containment technology) tool - different philosophy.

\todo{a new section: security experimentation}


\todo{look at CSET conference}

